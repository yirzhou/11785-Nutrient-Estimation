{"cells":[{"cell_type":"markdown","metadata":{"id":"D-QNNGMCMTY3"},"source":["# Midterm Expectation\n","\n","- Baseline\n","- Experiment (i.e., something different; e.x., spreadsheet)\n","- Timeline (what we have done, and what we will be doing)"]},{"cell_type":"markdown","metadata":{"id":"5NXFw1rhMTY7"},"source":["# Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"LnnqK5WMMTY7","executionInfo":{"status":"ok","timestamp":1651292320245,"user_tz":420,"elapsed":1022,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["import os\n","\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision.transforms import (\n","    CenterCrop,\n","    Compose,\n","    ToTensor,\n",")\n","from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","import math"]},{"cell_type":"markdown","source":["# Mount Google Drive using gdfuse"],"metadata":{"id":"msbOJn4yNcIq"}},{"cell_type":"code","source":["!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!sudo apt-get update -qq 2>&1 > /dev/null\n","!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n","!google-drive-ocamlfuse"],"metadata":{"id":"MAcPoTN7NgAZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"89fa71ae-c8c0-4846-980a-7286c682c85b","executionInfo":{"status":"ok","timestamp":1651292328639,"user_tz":420,"elapsed":8397,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n"]}]},{"cell_type":"code","source":["!sudo apt-get install -qq w3m # to act as web browser \n","!xdg-settings set default-web-browser w3m.desktop # to set default browser\n","%cd /content\n","!mkdir drive\n","%cd drive\n","!mkdir MyDrive\n","%cd ..\n","%cd ..\n","!google-drive-ocamlfuse /content/drive/MyDrive"],"metadata":{"id":"qdaiTTuiWqvV","executionInfo":{"status":"ok","timestamp":1651292334596,"user_tz":420,"elapsed":5968,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e89ea4b8-07dc-4e30-90f0-abeddaea9998"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","mkdir: cannot create directory ‘drive’: File exists\n","/content/drive\n","mkdir: cannot create directory ‘MyDrive’: File exists\n","/content\n","/\n","fuse: mountpoint is not empty\n","fuse: if you are sure this is safe, use the 'nonempty' mount option\n"]}]},{"cell_type":"markdown","source":["# Unzip Data"],"metadata":{"id":"28paTv3yY-E6"}},{"cell_type":"code","source":["# ! pip install gdown\n","# ! cd /content/ && gdown https://drive.google.com/uc?id=1zD7fUAt12L16ywPSjRsj4IqL-Lqg13zK\n","# # ! cp /content/drive/MyDrive/11785/project/data.zip /content/\n","# ! cd /content && unzip data.zip\n","# ! rm /content/data.zip"],"metadata":{"id":"gm5chDQyZBLO","executionInfo":{"status":"ok","timestamp":1651292334597,"user_tz":420,"elapsed":8,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n59ANLHGMTY9"},"source":["# Global Variables"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dJdqOG31MTY9","executionInfo":{"status":"ok","timestamp":1651292334597,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["DATA_PATH = \"/content/data\"\n","METADATA_DIR = f\"{DATA_PATH}/metadata\"\n","IMAGERY_DIR = f\"{DATA_PATH}/imagery/realsense_overhead\""]},{"cell_type":"markdown","metadata":{"id":"EKfNoKuIMTY9"},"source":["# Helper Functions for Data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Lbs8y5P5MTY-","executionInfo":{"status":"ok","timestamp":1651292334597,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["def read_csv_variable_cols(filepath: str) -> pd.DataFrame:\n","    \"\"\"https://stackoverflow.com/a/57824142.\n","    We only read the first 6 columns to retrieve required labels.\n","    \"\"\"\n","    ### Loop the data lines\n","    with open(filepath, 'r') as temp_f:\n","        # get No of columns in each line\n","        col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n","\n","    ### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n","    column_names = [i for i in range(0, max(col_count))]\n","\n","    ### Read csv\n","    return pd.read_csv(filepath, header=None, delimiter=\",\", names=column_names, low_memory=False).iloc[:,:6]"]},{"cell_type":"markdown","metadata":{"id":"L461g1bEMTY_"},"source":["# Ingredient and Dish Metadata (Groun Truths)"]},{"cell_type":"markdown","metadata":{"id":"TD2SzXIYMTY_"},"source":["## Data Format\n","\n","### Training-testing data\n","\n","- \"imagery/realsense_overhead/dish_<id>\" contains the images as the input data\n","- \"dish_ids\" contains training-testing splits\n","\n","### Labels (metadata)\n","\n","All labels need to be preprocessed. For each dish, we need to extract the following:\n","- total calorie\n","- mass (optional according to the paper)\n","- the amount of the three macronutrients (fat, carb, protein)\n","\n","It doesn't seem that bad - we don't need to process the ingredients because they are purely there for constructing the labels. For our multi-task learning, we only need to have the above labels. The three tasks are:\n","\n","1. Calorie\n","2. Macronutrients (fat, carb, protein)\n","3. Mass (optional)\n","\n","这样一来我们可以把labels和image data放在一起，每次返回input和expected output."]},{"cell_type":"markdown","metadata":{"id":"HxLtWU6WMTZA"},"source":["## Dish Metadata"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"cV6hUYHiMTZA","executionInfo":{"status":"ok","timestamp":1651292334598,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["# Metadata for dishes has variable numbers of columns per row.\n","# Can do similar stuff to dish_metadata_cafe2.csv\n","# The first 6 columns: [dish_id, total_calories, total_mass, total_fat, total_carb, total_protein]\n","dish_metadata_1 = read_csv_variable_cols(f\"{METADATA_DIR}/dish_metadata_cafe1.csv\")\n","# Rename the columns\n","dish_metadata_1 = dish_metadata_1.rename(columns={0:\"dish_id\", 1:\"total_calories\", 2:\"total_mass\", 3:\"total_fat\", 4:\"total_carb\", 5:\"total_protein\"})\n","\n","dish_metadata_2 = read_csv_variable_cols(f\"{METADATA_DIR}/dish_metadata_cafe2.csv\")\n","# Rename the columns\n","dish_metadata_2 = dish_metadata_2.rename(columns={0:\"dish_id\", 1:\"total_calories\", 2:\"total_mass\", 3:\"total_fat\", 4:\"total_carb\", 5:\"total_protein\"})\n","\n","dish_metadata = pd.concat((dish_metadata_1, dish_metadata_2), ignore_index=True)\n","# Convert to dictionary\n","labels_dict = dish_metadata.set_index(\"dish_id\").to_dict(\"index\")"]},{"cell_type":"markdown","metadata":{"id":"65hoT-vkMTZB"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1WszLEHvMTZB","executionInfo":{"status":"ok","timestamp":1651292334598,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["config = {\n","    'epochs': 150,\n","    'batch_size': 64,\n","    'lr': 1e-4,\n","}\n","\n","class Config:\n","    def __init__(self, config):\n","        for k, v in config.items():\n","            setattr(self, k, v)\n","\n","config = Config(config)"]},{"cell_type":"markdown","metadata":{"id":"gePyzvWIMTZB"},"source":["# Datasets and DataLoaders"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gYM_uPdMMTZC","executionInfo":{"status":"ok","timestamp":1651292334599,"user_tz":420,"elapsed":8,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["class RGBDataset(Dataset):\n","    \"\"\"4.2 The input resolution to the\n","    network is a 256x256 image, where images were downsized\n","    and center cropped in order to retain the most salient dish\n","    region.\n","\n","    我们baseline应该只用RGB就行 (根据4.2).\n","    \"\"\"\n","\n","    def __init__(self, data_dir, transforms=Compose([CenterCrop((256, 256)), ToTensor()]), labels=labels_dict, train=True):\n","        self.data_dir = data_dir\n","        self.transforms = transforms\n","        self.labels = labels\n","        self.train = train\n","\n","        # # ['dish_1556572657', 'dish_1556573514', 'dish_1556575014', 'dish_1556575083', 'dish_1556575124', 'dish_1556575273', 'dish_1556575327']\n","        dirs = os.listdir(self.data_dir)\n","\n","        self.dish_ids = []\n","        for dir in dirs:\n","            if \"rgb.png\" in os.listdir(os.path.join(self.data_dir,dir)):\n","                self.dish_ids.append(dir)\n","\n","        self.dish_ids.sort()\n","\n","        self.img_paths = list(\n","            map(\n","                lambda fname: os.path.join(self.data_dir, fname),\n","                self.dish_ids,\n","            )\n","        )\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        rgb_path = f\"{self.img_paths[idx]}/rgb.png\"\n","        dish_id = self.dish_ids[idx]\n","        transformed_img = self.transforms(Image.open(rgb_path))\n","        if self.train:\n","            label = torch.tensor(list(self.labels[dish_id].values()))\n","            return transformed_img, label\n","        else:\n","            return transformed_img"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"zDzPOZPxMTZC","executionInfo":{"status":"ok","timestamp":1651292334599,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["TRAIN_DIR = f\"{IMAGERY_DIR}/train\"\n","VALID_DIR = f\"{IMAGERY_DIR}/test\"\n","# TEST_DIR = IMAGERY_DIR\n","\n","train_dataset = RGBDataset(TRAIN_DIR, labels=labels_dict)\n","valid_dataset = RGBDataset(VALID_DIR, labels=labels_dict)\n","# test_dataset = RGBDataset(TEST_DIR, train=False)\n","\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n","valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n","# test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n","\n","num_training_batches = len(train_loader)"]},{"cell_type":"markdown","metadata":{"id":"5pAxIycIMTZD"},"source":["# InceptionV2"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jIkDGyD2MTZD","executionInfo":{"status":"ok","timestamp":1651292334890,"user_tz":420,"elapsed":298,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["def ConvBNReLU(in_channels,out_channels,kernel_size,stride=1,padding=0):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,padding=padding),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU6(inplace=True),\n","    )\n","\n","def ConvBNReLUFactorization(in_channels,out_channels,kernel_sizes,paddings):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_sizes, stride=1,padding=paddings),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU6(inplace=True)\n","    )\n","\n","class InceptionV2ModuleA(nn.Module):\n","    def __init__(self, in_channels,out_channels1,out_channels2reduce, out_channels2, out_channels3reduce, out_channels3, out_channels4):\n","        super(InceptionV2ModuleA, self).__init__()\n","\n","        self.branch1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels1,kernel_size=1)\n","\n","        self.branch2 = nn.Sequential(\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1),\n","            ConvBNReLU(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_size=3, padding=1),\n","        )\n","\n","        self.branch3 = nn.Sequential(\n","            ConvBNReLU(in_channels=in_channels,out_channels=out_channels3reduce,kernel_size=1),\n","            ConvBNReLU(in_channels=out_channels3reduce, out_channels=out_channels3, kernel_size=3, padding=1),\n","            ConvBNReLU(in_channels=out_channels3, out_channels=out_channels3, kernel_size=3, padding=1),\n","        )\n","\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels4, kernel_size=1),\n","        )\n","\n","    def forward(self, x):\n","        out1 = self.branch1(x)\n","        out2 = self.branch2(x)\n","        out3 = self.branch3(x)\n","        out4 = self.branch4(x)\n","        out = torch.cat([out1, out2, out3, out4], dim=1)\n","        return out\n","\n","class InceptionV2ModuleB(nn.Module):\n","    def __init__(self, in_channels,out_channels1,out_channels2reduce, out_channels2, out_channels3reduce, out_channels3, out_channels4):\n","        super(InceptionV2ModuleB, self).__init__()\n","\n","        self.branch1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels1,kernel_size=1)\n","\n","        self.branch2 = nn.Sequential(\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1),\n","            ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2reduce, kernel_sizes=[1,3],paddings=[0,1]),\n","            ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_sizes=[3,1],paddings=[1, 0]),\n","        )\n","\n","        self.branch3 = nn.Sequential(\n","            ConvBNReLU(in_channels=in_channels,out_channels=out_channels3reduce,kernel_size=1),\n","            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3reduce,kernel_sizes=[1, 3], paddings=[0, 1]),\n","            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3reduce,kernel_sizes=[3, 1], paddings=[1, 0]),\n","            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3reduce, kernel_sizes=[1, 3], paddings=[0, 1]),\n","            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3,kernel_sizes=[3, 1], paddings=[1, 0]),\n","        )\n","\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels4, kernel_size=1),\n","        )\n","\n","    def forward(self, x):\n","        out1 = self.branch1(x)\n","        out2 = self.branch2(x)\n","        out3 = self.branch3(x)\n","        out4 = self.branch4(x)\n","        out = torch.cat([out1, out2, out3, out4], dim=1)\n","        return out\n","\n","class InceptionV2ModuleC(nn.Module):\n","    def __init__(self, in_channels,out_channels1,out_channels2reduce, out_channels2, out_channels3reduce, out_channels3, out_channels4):\n","        super(InceptionV2ModuleC, self).__init__()\n","\n","        self.branch1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels1,kernel_size=1)\n","\n","        self.branch2_conv1 = ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1)\n","        self.branch2_conv2a = ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_sizes=[1,3],paddings=[0,1])\n","        self.branch2_conv2b = ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_sizes=[3,1],paddings=[1,0])\n","\n","        self.branch3_conv1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels3reduce,kernel_size=1)\n","        self.branch3_conv2 = ConvBNReLU(in_channels=out_channels3reduce, out_channels=out_channels3, kernel_size=3,stride=1,padding=1)\n","        self.branch3_conv3a = ConvBNReLUFactorization(in_channels=out_channels3, out_channels=out_channels3, kernel_sizes=[3, 1],paddings=[1, 0])\n","        self.branch3_conv3b = ConvBNReLUFactorization(in_channels=out_channels3, out_channels=out_channels3, kernel_sizes=[1, 3],paddings=[0, 1])\n","\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels4, kernel_size=1),\n","        )\n","\n","    def forward(self, x):\n","        out1 = self.branch1(x)\n","        x2 = self.branch2_conv1(x)\n","        out2 = torch.cat([self.branch2_conv2a(x2), self.branch2_conv2b(x2)],dim=1)\n","        x3 = self.branch3_conv2(self.branch3_conv1(x))\n","        out3 = torch.cat([self.branch3_conv3a(x3), self.branch3_conv3b(x3)], dim=1)\n","        out4 = self.branch4(x)\n","        out = torch.cat([out1, out2, out3, out4], dim=1)\n","        return out\n","\n","class InceptionV3ModuleD(nn.Module):\n","    def __init__(self, in_channels,out_channels1reduce,out_channels1,out_channels2reduce, out_channels2):\n","        super(InceptionV3ModuleD, self).__init__()\n","\n","        self.branch1 = nn.Sequential(\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels1reduce, kernel_size=1),\n","            ConvBNReLU(in_channels=out_channels1reduce, out_channels=out_channels1, kernel_size=3,stride=2,padding=1)\n","        )\n","\n","        self.branch2 = nn.Sequential(\n","            ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1),\n","            ConvBNReLU(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_size=3, stride=1, padding=1),\n","            ConvBNReLU(in_channels=out_channels2, out_channels=out_channels2, kernel_size=3, stride=2,padding=1),\n","        )\n","\n","        self.branch3 = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n","\n","    def forward(self, x):\n","        out1 = self.branch1(x)\n","        out2 = self.branch2(x)\n","        out3 = self.branch3(x)\n","        out = torch.cat([out1, out2, out3], dim=1)\n","        return out\n","\n","class InceptionAux(nn.Module):\n","    def __init__(self, in_channels,out_channels):\n","        super(InceptionAux, self).__init__()\n","\n","        self.auxiliary_avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n","        self.auxiliary_conv1 = ConvBNReLU(in_channels=in_channels, out_channels=128, kernel_size=1)\n","        self.auxiliary_conv2 = nn.Conv2d(in_channels=128, out_channels=768, kernel_size=5,stride=1)\n","        self.auxiliary_dropout = nn.Dropout(p=0.7)\n","        self.auxiliary_linear1 = nn.Linear(in_features=768, out_features=out_channels)\n","\n","    def forward(self, x):\n","        x = self.auxiliary_conv1(self.auxiliary_avgpool(x))\n","        x = self.auxiliary_conv2(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.auxiliary_linear1(self.auxiliary_dropout(x))\n","        return out\n","\n","class InceptionV2(nn.Module):\n","    def __init__(self, num_classes=1000, stage='train'):\n","        super(InceptionV2, self).__init__()\n","        self.stage = stage\n","\n","        self.block1 = nn.Sequential(\n","            ConvBNReLU(in_channels=3, out_channels=64, kernel_size=7,stride=2,padding=3),\n","            nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n","        )\n","\n","        self.block2 = nn.Sequential(\n","            ConvBNReLU(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1),\n","            nn.MaxPool2d(kernel_size=3, stride=2,padding=1),\n","        )\n","\n","        self.block3 = nn.Sequential(\n","            InceptionV2ModuleA(in_channels=192,out_channels1=64,out_channels2reduce=64, out_channels2=64, out_channels3reduce=64, out_channels3=96, out_channels4=32),\n","            InceptionV2ModuleA(in_channels=256, out_channels1=64, out_channels2reduce=64, out_channels2=96,out_channels3reduce=64, out_channels3=96, out_channels4=64),\n","            InceptionV3ModuleD(in_channels=320, out_channels1reduce=128, out_channels1=160, out_channels2reduce=64,out_channels2=96),\n","        )\n","\n","        self.block4 = nn.Sequential(\n","            InceptionV2ModuleB(in_channels=576, out_channels1=224, out_channels2reduce=64, out_channels2=96,out_channels3reduce=96, out_channels3=128, out_channels4=128),\n","            InceptionV2ModuleB(in_channels=576, out_channels1=192, out_channels2reduce=96, out_channels2=128,out_channels3reduce=96, out_channels3=128, out_channels4=128),\n","            InceptionV2ModuleB(in_channels=576, out_channels1=160, out_channels2reduce=128, out_channels2=160,out_channels3reduce=128, out_channels3=128, out_channels4=128),\n","            InceptionV2ModuleB(in_channels=576, out_channels1=96, out_channels2reduce=128, out_channels2=192,out_channels3reduce=160, out_channels3=160, out_channels4=128),\n","            InceptionV3ModuleD(in_channels=576, out_channels1reduce=128, out_channels1=192, out_channels2reduce=192,out_channels2=256),\n","        )\n","\n","        self.block5 = nn.Sequential(\n","            InceptionV2ModuleC(in_channels=1024, out_channels1=352, out_channels2reduce=192, out_channels2=160,out_channels3reduce=160, out_channels3=112, out_channels4=128),\n","            InceptionV2ModuleC(in_channels=1024, out_channels1=352, out_channels2reduce=192, out_channels2=160,\n","                               out_channels3reduce=192, out_channels3=112, out_channels4=128)\n","        )\n","\n","        self.avg_pool = nn.AdaptiveAvgPool2d((2,2))\n","        self.dropout = nn.Dropout(p=0.5)\n","    \n","\n","    def forward(self, x):\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = self.block4(x)\n","        x = self.block5(x)\n","        x = self.avg_pool(x)\n","        x = self.dropout(x)\n","        x = x.view(x.size(0), -1)\n","        return x"]},{"cell_type":"markdown","source":["# Resnet50"],"metadata":{"id":"U8khcs7eeugO"}},{"cell_type":"code","source":["import torch.utils.model_zoo as model_zoo\n","model_urls = {\n","    'resnet18': 'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://s3.amazonaws.com/pytorch/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://s3.amazonaws.com/pytorch/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://s3.amazonaws.com/pytorch/models/resnet152-b121ed2d.pth',\n","}\n","\n","class Bottleneck(nn.Module):\n","  expansion = 4\n","\n","  def __init__(self, inplanes, planes, stride=1, downsample=None):\n","    super(Bottleneck, self).__init__()\n","    \n","    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n","    self.bn1 = nn.BatchNorm2d(planes)\n","    \n","    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n","                 padding=1, bias=False)\n","    self.bn2 = nn.BatchNorm2d(planes)\n","    \n","    self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","    self.bn3 = nn.BatchNorm2d(planes * 4)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.downsample = downsample\n","    self.stride = stride\n","\n","  def forward(self, x):\n","    residual = x\n","\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out = self.relu(out)\n","\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","    out = self.relu(out)\n","\n","    out = self.conv3(out)\n","    out = self.bn3(out)\n","\n","    if self.downsample is not None:\n","      residual = self.downsample(x)\n","\n","    out += residual\n","    out = self.relu(out)\n","\n","    return out\n","    \n","    \n","class ResNet(nn.Module):\n","  def __init__(self, block, layers, num_classes=1000):\n","    self.inplanes = 64\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                 bias=False)\n","    self.bn1 = nn.BatchNorm2d(64)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # change\n","    self.layer1 = self._make_layer(block, 64, layers[0])\n","    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)   # different\n","    self.avgpool = nn.AvgPool2d(7)\n","    self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","    for m in self.modules():\n","      if isinstance(m, nn.Conv2d):\n","        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","        m.weight.data.normal_(0, math.sqrt(2. / n))\n","      elif isinstance(m, nn.BatchNorm2d):\n","        m.weight.data.fill_(1)\n","        m.bias.data.zero_()\n","\n","  def _make_layer(self, block, planes, blocks, stride=1):\n","    downsample = None\n","    if stride != 1 or self.inplanes != planes * block.expansion:\n","      downsample = nn.Sequential(\n","        nn.Conv2d(self.inplanes, planes * block.expansion,\n","              kernel_size=1, stride=stride, bias=False),\n","        nn.BatchNorm2d(planes * block.expansion),\n","      )\n","\n","    layers = []\n","    layers.append(block(self.inplanes, planes, stride, downsample))\n","    self.inplanes = planes * block.expansion\n","    for i in range(1, blocks):\n","      layers.append(block(self.inplanes, planes))\n","\n","    return nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = self.bn1(x)\n","    x = self.relu(x)\n","    x = self.maxpool(x)\n","\n","    x = self.layer1(x)\n","    x = self.layer2(x)\n","    x = self.layer3(x)\n","    x = self.layer4(x)\n","\n","    x = self.avgpool(x)\n","    x = x.view(x.size(0), -1)\n","    x = self.fc(x)\n","\n","    return x\n","\n","def resnet50(pretrained=True):\n","  \"\"\"Constructs a ResNet-50 model.\n","  Args:\n","    pretrained (bool): If True, returns a model pre-trained on ImageNet\n","  \"\"\"\n","  model = ResNet(Bottleneck, [3, 4, 6, 3])\n","  if pretrained:\n","    model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n","  return model\n","\n","def resnet101(pretrained=True):\n","  \"\"\"Constructs a ResNet-50 model.\n","  Args:\n","    pretrained (bool): If True, returns a model pre-trained on ImageNet\n","  \"\"\"\n","  model = ResNet(Bottleneck, [3, 4, 23, 3])\n","  if pretrained:\n","    model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n","  return model\n","\n","\n","def resnet152(pretrained=True):\n","  \"\"\"Constructs a ResNet-152 model.\n","  Args:\n","    pretrained (bool): If True, returns a model pre-trained on ImageNet\n","  \"\"\"\n","  model = ResNet(Bottleneck, [3, 8, 36, 3])\n","  if pretrained:\n","    model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n","  return model\n"],"metadata":{"id":"YvKzmQ-Cep_U","executionInfo":{"status":"ok","timestamp":1651292335412,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iDmn9JoMTZF"},"source":["# Baseline Model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"psQWvO4fMTZG","executionInfo":{"status":"ok","timestamp":1651292335413,"user_tz":420,"elapsed":7,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["class BaseNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # self.backbone = InceptionV2([1, 1, 3])\n","        self.backbone = resnet101()\n","        self.fc = nn.Linear(1000, 4096)\n","        self.fc1 = nn.Linear(4096, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc_calories = nn.Sequential(\n","            nn.Linear(4096, 4096),\n","            nn.Linear(4096, 1)\n","        )\n","        self.fc_mass = nn.Sequential(\n","            nn.Linear(4096, 4096),\n","            nn.Linear(4096, 1)\n","        )\n","        self.fc_mc = nn.Sequential(\n","            nn.Linear(4096, 4096),\n","            nn.Linear(4096, 3)\n","        )\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = self.fc(x)\n","        x = self.fc2(self.fc1(x))\n","\n","        x_cal = self.fc_calories(x)\n","        x_mass = self.fc_mass(x)\n","        x_mn = self.fc_mc(x)\n","\n","        return x_cal, x_mass, x_mn"]},{"cell_type":"markdown","metadata":{"id":"5taNW4uyMTZG"},"source":["# Multi-task Loss"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Hmt_30p0MTZG","executionInfo":{"status":"ok","timestamp":1651292335413,"user_tz":420,"elapsed":6,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[],"source":["class MultiTaskLearner(nn.Module):\n","    def __init__(self, model: nn.Module):\n","        super(MultiTaskLearner, self).__init__()\n","        self.model = model\n","        self.criterion = nn.L1Loss()\n","\n","    def forward(self, x, y):\n","        # 1 x 5 Tensor [total_calories, total_mass, total_fat, total_carb, total_protein]\n","\n","        out_cal, out_mass, out_mn = self.model(x)\n","\n","        loss_calorie = self.criterion(out_cal, y[:, 0:1])\n","        \n","        loss_mass = self.criterion(out_mass, y[:, 1:2])\n","\n","        loss_mn = self.criterion(out_mn, y[:, 2:])\n","\n","        loss_total = loss_calorie + loss_mass + loss_mn\n","\n","        return loss_total"]},{"cell_type":"markdown","source":["# Delete Model"],"metadata":{"id":"NAGwpGjhfhSv"}},{"cell_type":"code","source":["# del model"],"metadata":{"id":"DZCilCtqfgos","executionInfo":{"status":"ok","timestamp":1651292335413,"user_tz":420,"elapsed":6,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Utility Funs"],"metadata":{"id":"SwidA51rTCTK"}},{"cell_type":"code","source":["CHECKPOINT_PATH = \"/content/drive/MyDrive/checkpoints\"\n","def create_dir_if_not_exists(dirpath: str):\n","    \"\"\"Create the specified directory with all intermediate directories if necessary.\"\"\"\n","    if not os.path.exists(dirpath):\n","        os.makedirs(dirpath)\n","\n","def save_checkpoint(\n","    epoch: int,\n","    loss: float,\n","    model: nn.Module,\n","    model_name: str,\n","    checkpoint_path: str = CHECKPOINT_PATH,\n","):\n","    create_dir_if_not_exists(checkpoint_path)\n","    torch.save(\n","        {\"epoch\": epoch, \"loss\": loss, \"model_state_dict\": model.state_dict()},\n","        os.path.join(checkpoint_path, model_name),\n","    )\n","\n","\n","def load_checkpoint(filepath: str):\n","    state_dict = torch.load(filepath)\n","    epoch, loss, model_state_dict = (\n","        state_dict[\"epoch\"],\n","        state_dict[\"loss\"],\n","        state_dict[\"model_state_dict\"],\n","    )\n","    return epoch, loss, model_state_dict"],"metadata":{"id":"MBVxtHj6TEE-","executionInfo":{"status":"ok","timestamp":1651292335414,"user_tz":420,"elapsed":6,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"XUHFw2ZWSj1U"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"qXEIbyroMTZH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e3fe6e4-a069-4ca4-f0bd-7b715ffe6349","executionInfo":{"status":"ok","timestamp":1651292341187,"user_tz":420,"elapsed":5779,"user":{"displayName":"Ricardo Wang","userId":"08964684308248908958"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda is available: True\n"]}],"source":["print(f\"Cuda is available: {torch.cuda.is_available()}\")\n","model = BaseNet()\n","model.cuda()\n","learner = MultiTaskLearner(model)\n","\n","optimizer = torch.optim.RMSprop(model.parameters(), config.lr, momentum=0.9, weight_decay=0.9, eps=1.0)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs*len(train_loader))\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cONTwe7yMTZH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac643fda-defa-4e63-92c5-6fc9824687d1"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rTrain:   0%|          | 0/41 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n","  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/150: Train Loss 452.8264, Learning Rate 0.0001, Valid Loss 459.9333\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 2/150: Train Loss 274.1188, Learning Rate 0.0001, Valid Loss 254.4033\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 3/150: Train Loss 171.4869, Learning Rate 0.0001, Valid Loss 226.0134\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 4/150: Train Loss 145.6760, Learning Rate 0.0001, Valid Loss 275.3047\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 5/150: Train Loss 142.1922, Learning Rate 0.0001, Valid Loss 211.3851\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 6/150: Train Loss 127.4104, Learning Rate 0.0001, Valid Loss 259.9371\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 7/150: Train Loss 123.7825, Learning Rate 0.0001, Valid Loss 220.2713\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 8/150: Train Loss 109.6245, Learning Rate 0.0001, Valid Loss 219.0507\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 9/150: Train Loss 104.0472, Learning Rate 0.0001, Valid Loss 199.7566\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 10/150: Train Loss 101.2070, Learning Rate 0.0001, Valid Loss 219.7648\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 11/150: Train Loss 93.3186, Learning Rate 0.0001, Valid Loss 212.4523\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 12/150: Train Loss 86.0454, Learning Rate 0.0001, Valid Loss 220.9445\n"]},{"output_type":"stream","name":"stderr","text":["Train:  15%|█▍        | 6/41 [00:09<00:51,  1.47s/it, loss=85.1512, lr=0.0001]"]}],"source":["for epoch in range(config.epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    for i, (x, y) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        x = x.cuda()\n","        y = y.cuda()\n","\n","        # with torch.cuda.amp.autocast():     \n","        loss = learner(x, y)\n","\n","        # Update # correct & loss as we go\n","        total_loss += float(loss)\n","\n","        # Compute training metrics\n","        train_loss = float(total_loss / (i + 1))\n","        cur_lr = float(optimizer.param_groups[0]['lr'])\n","\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            loss=\"{:.04f}\".format(train_loss),\n","            lr=\"{:.04f}\".format(cur_lr))\n","        \n","        # Another couple things you need for FP16. \n","        # scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        loss.backward()\n","        # scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        optimizer.step()\n","        # scaler.update() # This is something added just for FP16\n","\n","        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    train_loss = total_loss / len(train_loader)\n","    \n","    # Save the model every 3 epochs\n","    if epoch % 3 == 0:\n","        save_checkpoint(epoch, train_loss, model, \"baseline\")\n","\n","    # You can add validation per-epoch here if you would like\n","    model.eval()\n","    batch_bar = tqdm(total=len(valid_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","    total_loss = 0\n","    for i, (x, y) in enumerate(valid_loader):\n","\n","        x = x.cuda()\n","        y = y.cuda()\n","\n","        with torch.no_grad():\n","            loss = learner(x, y)\n","        \n","\n","        total_loss += float(loss)\n","\n","        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n","\n","        batch_bar.update()\n","        \n","    batch_bar.close()\n","\n","    # scheduler.step(float(total_loss / (i + 1)))\n","\n","    valid_loss = total_loss / len(valid_loader)\n","\n","    print(\"Epoch {}/{}: Train Loss {:.04f}, Learning Rate {:.04f}, Valid Loss {:.04f}\".format(\n","        epoch + 1, config.epochs, train_loss, cur_lr, valid_loss))\n"]},{"cell_type":"markdown","source":["## Inference on training and testing data"],"metadata":{"id":"_1uptrMwsx_Q"}},{"cell_type":"code","source":["prediction_filepath = \"/content/data/metadata/outputs.csv\"\n","# if os.path.exists(prediction_filepath):\n","#     os.remove(prediction_filepath)\n","\n","model.eval()\n","batch_bar = tqdm(total=len(eval_train_loader), dynamic_ncols=True, position=0, leave=False, desc='Eval_Train')\n","results_all = None\n","# Inference on training data\n","for i, (x, y) in enumerate(eval_train_loader):\n","\n","    x = x.cuda()\n","    dish_ids = np.array(list(y))\n","    dish_ids = dish_ids.reshape(dish_ids.shape[0],1)\n","\n","    cal, mass, mn = model(x)\n","\n","    results = torch.cat((cal,mass,mn), 1).detach().cpu().numpy()\n","\n","    results = np.concatenate((dish_ids, results), 1)\n","    \n","    if results_all is None:\n","        results_all = results\n","    else:\n","        results_all = np.concatenate((results_all, results), 0)\n","\n","    del cal, mass, mn\n","    torch.cuda.empty_cache()\n","\n","    batch_bar.update()\n","    \n","batch_bar.close()\n","torch.cuda.empty_cache()\n","# Inference on validation data\n","batch_bar = tqdm(total=len(eval_val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","for i, (x, y) in enumerate(eval_val_loader):\n","\n","    x = x.cuda()\n","    dish_ids = np.array(list(y))\n","    dish_ids = dish_ids.reshape(dish_ids.shape[0],1)\n","\n","    cal, mass, mn = model(x)\n","\n","    results = torch.cat((cal,mass,mn), 1).detach().cpu().numpy()\n","\n","    results = np.concatenate((dish_ids, results), 1)\n","    \n","    if results_all is None:\n","        results_all = results\n","    else:\n","        results_all = np.concatenate((results_all, results), 0)\n","\n","    del cal, mass, mn\n","    torch.cuda.empty_cache()\n","\n","    batch_bar.update()\n","    \n","batch_bar.close()"],"metadata":{"id":"346Ik9qrss9-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write to csv\n","np.savetxt(prediction_filepath, results_all, delimiter=\",\", fmt='%s,%s,%s,%s,%s,%s')"],"metadata":{"id":"YmWpoEeDs3uM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r\"\"\"Script to compute statistics on nutrition predictions.\n","\n","This script takes in a csv of nutrition predictions and computes absolute and\n","percentage mean average error values comparable to the metrics used to eval\n","models in the Nutrition5k paper. The input csv file of nutrition predictions\n","should be in the form of:\n","dish_id, calories, mass, carbs, protein\n","And the groundtruth values will be pulled from the metadata csv file provided\n","in the Nutrition5k dataset release where the first 5 fields are also:\n","dish_id, calories, mass, carbs, protein\n","\n","Example Usage:\n","python compute_statistics.py path/to/groundtruth.csv path/to/predictions.csv \\\n","path/to/output_statistics.json\n","\"\"\"\n","\n","import json\n","from os import path\n","import statistics\n","import sys\n","\n","DISH_ID_INDEX = 0\n","DATA_FIELDNAMES = [\"dish_id\", \"calories\", \"mass\", \"fat\", \"carb\", \"protein\"]\n","\n","\n","def ReadCsvData(filepath):\n","  if not path.exists(filepath):\n","    raise Exception(\"File %s not found\" % path)\n","  parsed_data = {}\n","  with open(filepath, \"r\") as f_in:\n","    filelines = f_in.readlines()\n","    for line in filelines:\n","      data_values = line.strip().split(\",\")\n","      parsed_data[data_values[DISH_ID_INDEX]] = data_values\n","  return parsed_data\n","\n","# groundtruth_csv_path = \"/content/data/metadata/ground_truth.csv\"\n","groundtruth_csv_path = \"/content/drive/MyDrive/11785/project/ground_truth.csv\"\n","predictions_csv_path = prediction_filepath\n","output_path = \"/content/data/metadata/eval_results.json\"\n","\n","groundtruth_data = ReadCsvData(groundtruth_csv_path)\n","prediction_data = ReadCsvData(predictions_csv_path)\n","\n","groundtruth_values = {}\n","err_values = {}\n","output_stats = {}\n","\n","for field in DATA_FIELDNAMES[1:]:\n","  groundtruth_values[field] = []\n","  err_values[field] = []\n","\n","for dish_id in prediction_data:\n","  for i in range(1, len(DATA_FIELDNAMES)):\n","    groundtruth_values[DATA_FIELDNAMES[i]].append(\n","        float(groundtruth_data[dish_id][i]))\n","    err_values[DATA_FIELDNAMES[i]].append(abs(\n","        float(prediction_data[dish_id][i])\n","        - float(groundtruth_data[dish_id][i])))\n","\n","for field in DATA_FIELDNAMES[1:]:\n","  output_stats[field + \"_MAE\"] = statistics.mean(err_values[field])\n","  output_stats[field + \"_MAE_%\"] = (100 * statistics.mean(err_values[field]) /\n","                                    statistics.mean(groundtruth_values[field]))\n","\n","with open(output_path, \"w\") as f_out:\n","  f_out.write(json.dumps(output_stats))\n"],"metadata":{"id":"aSVoe16_s5lG"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"af28af081fbc6591a0657d60a0d8fa83ec1369d1ecf4b5402213d8282a449a4e"},"kernelspec":{"display_name":"Python 3.9.10 ('intro2dl')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4,"colab":{"name":"nutrition5k.ipynb","provenance":[],"collapsed_sections":["D-QNNGMCMTY3","5NXFw1rhMTY7","msbOJn4yNcIq","28paTv3yY-E6","n59ANLHGMTY9","EKfNoKuIMTY9","L461g1bEMTY_","TD2SzXIYMTY_","HxLtWU6WMTZA","65hoT-vkMTZB","5pAxIycIMTZD","8iDmn9JoMTZF","5taNW4uyMTZG","NAGwpGjhfhSv","SwidA51rTCTK"],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}