{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    ToTensor,\n",
    ")\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./../sample_data\"\n",
    "METADATA_DIR = f\"{DATA_PATH}/metadata\"\n",
    "IMAGERY_DIR = f\"{DATA_PATH}/imagery/realsense_overhead\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_variable_cols(filepath: str):\n",
    "    \"\"\"https://stackoverflow.com/a/57824142\"\"\"\n",
    "    ### Loop the data lines\n",
    "    with open(filepath, 'r') as temp_f:\n",
    "        # get No of columns in each line\n",
    "        col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "\n",
    "    ### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "    column_names = [i for i in range(0, max(col_count))]\n",
    "\n",
    "    ### Read csv\n",
    "    return pd.read_csv(filepath, header=None, delimiter=\",\", names=column_names, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient and Dish Metadata (Groun Truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_metadata = pd.read_csv(f\"{METADATA_DIR}/ingredients_metadata.csv\", names=[\"name\", \"id\", \"cal\", \"fat\", \"carb\", \"protein\"], skiprows=1)\n",
    "print(ingredient_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata for dishes has variable numbers of columns per row.\n",
    "# Can do similar stuff to dish_metadata_cafe2.csv\n",
    "dish_metadata_1 = read_csv_variable_cols(f\"{METADATA_DIR}/dish_metadata_cafe1.csv\")\n",
    "print(dish_metadata_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBDataset(Dataset):\n",
    "    \"\"\"4.2 The input resolution to the\n",
    "    network is a 256x256 image, where images were downsized\n",
    "    and center cropped in order to retain the most salient dish\n",
    "    region.\n",
    "\n",
    "    我们baseline应该只用RGB就行 (根据4.2).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Also return the metadata here?\n",
    "    def __init__(self, data_dir, transforms=Compose([CenterCrop((256, 256)), ToTensor()])):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.img_paths = list(\n",
    "            map(\n",
    "                lambda fname: os.path.join(self.data_dir, fname),\n",
    "                sorted(os.listdir(self.data_dir)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgd_path = f\"{self.img_paths[idx]}/rgb.png\"\n",
    "        return self.transforms(Image.open(rgd_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "rgb_dataset = RGBDataset(IMAGERY_DIR)\n",
    "for data in rgb_dataset:\n",
    "    print(data.shape)\n",
    "    # torch.Size([3, 256, 256])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af28af081fbc6591a0657d60a0d8fa83ec1369d1ecf4b5402213d8282a449a4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('intro2dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
