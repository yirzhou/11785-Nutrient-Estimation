{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Expectation\n",
    "\n",
    "- Baseline\n",
    "- Experiment (i.e., something different; e.x., spreadsheet)\n",
    "- Timeline (what we have done, and what we will be doing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    ToTensor,\n",
    ")\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./../sample_data\"\n",
    "METADATA_DIR = f\"{DATA_PATH}/metadata\"\n",
    "IMAGERY_DIR = f\"{DATA_PATH}/imagery/realsense_overhead\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_variable_cols(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"https://stackoverflow.com/a/57824142.\n",
    "    We only read the first 6 columns to retrieve required labels.\n",
    "    \"\"\"\n",
    "    ### Loop the data lines\n",
    "    with open(filepath, 'r') as temp_f:\n",
    "        # get No of columns in each line\n",
    "        col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "\n",
    "    ### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "    column_names = [i for i in range(0, max(col_count))]\n",
    "\n",
    "    ### Read csv\n",
    "    return pd.read_csv(filepath, header=None, delimiter=\",\", names=column_names, low_memory=False).iloc[:,:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient and Dish Metadata (Groun Truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "\n",
    "### Training-testing data\n",
    "\n",
    "- \"imagery/realsense_overhead/dish_<id>\" contains the images as the input data\n",
    "- \"dish_ids\" contains training-testing splits\n",
    "\n",
    "### Labels (metadata)\n",
    "\n",
    "All labels need to be preprocessed. For each dish, we need to extract the following:\n",
    "- total calorie\n",
    "- mass (optional according to the paper)\n",
    "- the amount of the three macronutrients (fat, carb, protein)\n",
    "\n",
    "It doesn't seem that bad - we don't need to process the ingredients because they are purely there for constructing the labels. For our multi-task learning, we only need to have the above labels. The three tasks are:\n",
    "\n",
    "1. Calorie\n",
    "2. Macronutrients (fat, carb, protein)\n",
    "3. Mass (optional)\n",
    "\n",
    "这样一来我们可以把labels和image data放在一起，每次返回input和expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients Metadata (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_metadata = pd.read_csv(f\"{METADATA_DIR}/ingredients_metadata.csv\", names=[\"name\", \"id\", \"cal\", \"fat\", \"carb\", \"protein\"], skiprows=1)\n",
    "print(ingredient_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dish Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata for dishes has variable numbers of columns per row.\n",
    "# Can do similar stuff to dish_metadata_cafe2.csv\n",
    "# The first 6 columns: [dish_id, total_calories, total_mass, total_fat, total_carb, total_protein]\n",
    "dish_metadata_1 = read_csv_variable_cols(f\"{METADATA_DIR}/dish_metadata_cafe1.csv\")\n",
    "# Rename the columns\n",
    "dish_metadata_1 = dish_metadata_1.rename(columns={0:\"dish_id\", 1:\"total_calories\", 2:\"total_mass\", 3:\"total_fat\", 4:\"total_carb\", 5:\"total_protein\"})\n",
    "print(dish_metadata_1.iloc[:10,:])\n",
    "\n",
    "# Convert to dictionary\n",
    "labels_dict = dish_metadata_1.set_index(\"dish_id\").to_dict(\"index\")\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBDataset(Dataset):\n",
    "    \"\"\"4.2 The input resolution to the\n",
    "    network is a 256x256 image, where images were downsized\n",
    "    and center cropped in order to retain the most salient dish\n",
    "    region.\n",
    "\n",
    "    我们baseline应该只用RGB就行 (根据4.2).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Also return the metadata here?\n",
    "    def __init__(self, data_dir, transforms=Compose([CenterCrop((256, 256)), ToTensor()]), labels=labels_dict):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.labels = labels\n",
    "\n",
    "        # # ['dish_1556572657', 'dish_1556573514', 'dish_1556575014', 'dish_1556575083', 'dish_1556575124', 'dish_1556575273', 'dish_1556575327']\n",
    "        self.dish_ids = sorted(os.listdir(self.data_dir))\n",
    "\n",
    "        self.img_paths = list(\n",
    "            map(\n",
    "                lambda fname: os.path.join(self.data_dir, fname),\n",
    "                self.dish_ids,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgd_path = f\"{self.img_paths[idx]}/rgb.png\"\n",
    "        dish_id = self.dish_ids[idx]\n",
    "        label = torch.tensor(list(self.labels[dish_id].values()))\n",
    "        return self.transforms(Image.open(rgd_path)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_dataset = RGBDataset(IMAGERY_DIR)\n",
    "for x, y in rgb_dataset:\n",
    "    print(x, y)\n",
    "    # torch.Size([3, 256, 256])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBNReLU(in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(inplace=True),\n",
    "    )\n",
    "\n",
    "def ConvBNReLUFactorization(in_channels,out_channels,kernel_sizes,paddings):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_sizes, stride=1,padding=paddings),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class InceptionV2ModuleA(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels1,out_channels2reduce, out_channels2, out_channels3reduce, out_channels3, out_channels4):\n",
    "        super(InceptionV2ModuleA, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels1,kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1),\n",
    "            ConvBNReLU(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels,out_channels=out_channels3reduce,kernel_size=1),\n",
    "            ConvBNReLU(in_channels=out_channels3reduce, out_channels=out_channels3, kernel_size=3, padding=1),\n",
    "            ConvBNReLU(in_channels=out_channels3, out_channels=out_channels3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        out = torch.cat([out1, out2, out3, out4], dim=1)\n",
    "        return out\n",
    "\n",
    "class InceptionV2ModuleB(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels1,out_channels2reduce, out_channels2, out_channels3reduce, out_channels3, out_channels4):\n",
    "        super(InceptionV2ModuleB, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels1,kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1),\n",
    "            ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2reduce, kernel_sizes=[1,3],paddings=[0,1]),\n",
    "            ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_sizes=[3,1],paddings=[1, 0]),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels,out_channels=out_channels3reduce,kernel_size=1),\n",
    "            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3reduce,kernel_sizes=[1, 3], paddings=[0, 1]),\n",
    "            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3reduce,kernel_sizes=[3, 1], paddings=[1, 0]),\n",
    "            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3reduce, kernel_sizes=[1, 3], paddings=[0, 1]),\n",
    "            ConvBNReLUFactorization(in_channels=out_channels3reduce, out_channels=out_channels3,kernel_sizes=[3, 1], paddings=[1, 0]),\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        out = torch.cat([out1, out2, out3, out4], dim=1)\n",
    "        return out\n",
    "\n",
    "class InceptionV2ModuleC(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels1,out_channels2reduce, out_channels2, out_channels3reduce, out_channels3, out_channels4):\n",
    "        super(InceptionV2ModuleC, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels1,kernel_size=1)\n",
    "\n",
    "        self.branch2_conv1 = ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1)\n",
    "        self.branch2_conv2a = ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_sizes=[1,3],paddings=[0,1])\n",
    "        self.branch2_conv2b = ConvBNReLUFactorization(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_sizes=[3,1],paddings=[1,0])\n",
    "\n",
    "        self.branch3_conv1 = ConvBNReLU(in_channels=in_channels,out_channels=out_channels3reduce,kernel_size=1)\n",
    "        self.branch3_conv2 = ConvBNReLU(in_channels=out_channels3reduce, out_channels=out_channels3, kernel_size=3,stride=1,padding=1)\n",
    "        self.branch3_conv3a = ConvBNReLUFactorization(in_channels=out_channels3, out_channels=out_channels3, kernel_sizes=[3, 1],paddings=[1, 0])\n",
    "        self.branch3_conv3b = ConvBNReLUFactorization(in_channels=out_channels3, out_channels=out_channels3, kernel_sizes=[1, 3],paddings=[0, 1])\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        x2 = self.branch2_conv1(x)\n",
    "        out2 = torch.cat([self.branch2_conv2a(x2), self.branch2_conv2b(x2)],dim=1)\n",
    "        x3 = self.branch3_conv2(self.branch3_conv1(x))\n",
    "        out3 = torch.cat([self.branch3_conv3a(x3), self.branch3_conv3b(x3)], dim=1)\n",
    "        out4 = self.branch4(x)\n",
    "        out = torch.cat([out1, out2, out3, out4], dim=1)\n",
    "        return out\n",
    "\n",
    "class InceptionV3ModuleD(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels1reduce,out_channels1,out_channels2reduce, out_channels2):\n",
    "        super(InceptionV3ModuleD, self).__init__()\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels1reduce, kernel_size=1),\n",
    "            ConvBNReLU(in_channels=out_channels1reduce, out_channels=out_channels1, kernel_size=3,stride=2,padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=out_channels2reduce, kernel_size=1),\n",
    "            ConvBNReLU(in_channels=out_channels2reduce, out_channels=out_channels2, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNReLU(in_channels=out_channels2, out_channels=out_channels2, kernel_size=3, stride=2,padding=1),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out = torch.cat([out1, out2, out3], dim=1)\n",
    "        return out\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels):\n",
    "        super(InceptionAux, self).__init__()\n",
    "\n",
    "        self.auxiliary_avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.auxiliary_conv1 = ConvBNReLU(in_channels=in_channels, out_channels=128, kernel_size=1)\n",
    "        self.auxiliary_conv2 = nn.Conv2d(in_channels=128, out_channels=768, kernel_size=5,stride=1)\n",
    "        self.auxiliary_dropout = nn.Dropout(p=0.7)\n",
    "        self.auxiliary_linear1 = nn.Linear(in_features=768, out_features=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.auxiliary_conv1(self.auxiliary_avgpool(x))\n",
    "        x = self.auxiliary_conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.auxiliary_linear1(self.auxiliary_dropout(x))\n",
    "        return out\n",
    "\n",
    "class InceptionV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, stage='train'):\n",
    "        super(InceptionV2, self).__init__()\n",
    "        self.stage = stage\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=3, out_channels=64, kernel_size=7,stride=2,padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2,padding=1),\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            InceptionV2ModuleA(in_channels=192,out_channels1=64,out_channels2reduce=64, out_channels2=64, out_channels3reduce=64, out_channels3=96, out_channels4=32),\n",
    "            InceptionV2ModuleA(in_channels=256, out_channels1=64, out_channels2reduce=64, out_channels2=96,out_channels3reduce=64, out_channels3=96, out_channels4=64),\n",
    "            InceptionV3ModuleD(in_channels=320, out_channels1reduce=128, out_channels1=160, out_channels2reduce=64,out_channels2=96),\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            InceptionV2ModuleB(in_channels=576, out_channels1=224, out_channels2reduce=64, out_channels2=96,out_channels3reduce=96, out_channels3=128, out_channels4=128),\n",
    "            InceptionV2ModuleB(in_channels=576, out_channels1=192, out_channels2reduce=96, out_channels2=128,out_channels3reduce=96, out_channels3=128, out_channels4=128),\n",
    "            InceptionV2ModuleB(in_channels=576, out_channels1=160, out_channels2reduce=128, out_channels2=160,out_channels3reduce=128, out_channels3=128, out_channels4=128),\n",
    "            InceptionV2ModuleB(in_channels=576, out_channels1=96, out_channels2reduce=128, out_channels2=192,out_channels3reduce=160, out_channels3=160, out_channels4=128),\n",
    "            InceptionV3ModuleD(in_channels=576, out_channels1reduce=128, out_channels1=192, out_channels2reduce=192,out_channels2=256),\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            InceptionV2ModuleC(in_channels=1024, out_channels1=352, out_channels2reduce=192, out_channels2=160,out_channels3reduce=160, out_channels3=112, out_channels4=128),\n",
    "            InceptionV2ModuleC(in_channels=1024, out_channels1=352, out_channels2reduce=192, out_channels2=160,\n",
    "                               out_channels3reduce=192, out_channels3=112, out_channels4=128)\n",
    "        )\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "if __name__=='__main__':\n",
    "    model = InceptionV2()\n",
    "    print(model)\n",
    "\n",
    "    input = torch.randn(1, 3, 256, 256)\n",
    "    out = model(input)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = InceptionV2([1, 1, 3])\n",
    "        self.fc1 = nn.Linear(4096, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc_calories = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.Linear(4096, 1)\n",
    "        )\n",
    "        self.fc_mass = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.Linear(4096, 1)\n",
    "        )\n",
    "        self.fc_mc = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.Linear(4096, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "\n",
    "        x_cal = self.fc_calories(x)\n",
    "        x_mass = self.fc_mass(x)\n",
    "        x_mn = self.fc_mc(x)\n",
    "\n",
    "        return x_cal, x_mass, x_mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLearner(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super(MultiTaskLearner, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 1 x 5 Tensor [total_calories, total_mass, total_fat, total_carb, total_protein]\n",
    "        # [0] - total_calories \n",
    "\n",
    "        loss_calorie = self.criterion(x[0], y[0])\n",
    "        \n",
    "        loss_mass = self.criterion(x[1], y[1])\n",
    "\n",
    "        loss_mn = self.criterion(x[2], y[2])\n",
    "\n",
    "        loss_total = loss_calorie + loss_mass + loss_mn\n",
    "\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseNet()\n",
    "learner = MultiTaskLearner(model)\n",
    "input = torch.randn(1, 3, 256, 256)\n",
    "out = model(input)\n",
    "print(out[0].shape, out[1].shape, out[2].shape)\n",
    "\n",
    "loss = learner(out, [torch.Tensor([[1]]),torch.Tensor([[2]]),torch.Tensor([[3,4,5]])])\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af28af081fbc6591a0657d60a0d8fa83ec1369d1ecf4b5402213d8282a449a4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('intro2dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
